# -*- coding: utf-8 -*-
"""Final_course_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wLMow1E-o9B8FgyQr8eLQgVyxdWSW_N5

# Project - Fake News Detection

## Installing modules
"""

!pip install nltk
!pip install wordcloud
!pip install import-ipynb
!pip install -r requirements.txt

"""## Loading modules"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import scipy.sparse
import matplotlib.pyplot as plt
import joblib
from sklearn import preprocessing
from collections import Counter
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
import string as st #for text pre-processing
import re #Regular expression operations
import nltk
from nltk.corpus import stopwords
from nltk import tokenize
from nltk import ngrams
from nltk import bigrams
from nltk import trigrams
from nltk.stem import WordNetLemmatizer
import statistics
from statistics import mean
import os
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity as cs
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
import seaborn as sb
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
import import_ipynb

from sklearn.metrics.pairwise import cosine_similarity

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import make_scorer, confusion_matrix
from sklearn.model_selection import learning_curve

import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from keras.models import Model
from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.models import load_model
# %matplotlib inline


# Custom settings to view all column names and their data in the output
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', -1)

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

"""## Defining file paths"""

METADATA_FILEPATH = "train_data/train.json"
ARTICLES_FILEPATH = "train_data/train_articles"
DATASET_FOLDER = "train_data"
DATASET_ZIP = "train.zip"
PREDICTIONS_FILEPATH = "predictions.txt"
PREPROCESSED_ARTICLES = "preprocessed_articles.csv"
STOPWORDS_FILEPATH = "stop_words.txt"
ALL_PREPROCESSED = "all_preprocessed_final.csv"

"""## Configuring Notebook

**Please note that the following settings have been configured to run this notebook with preprocessed and pretrained data so that it could be run in the least amount of time!!**
"""

enablePreprocessing = 0
enableTraining = 0
enableWordCloudGeneration = 0
# BERT is set to false as it takes a lot of time to run. Please read the cells only
runBERT = 0

"""## Extracting dataset"""

import zipfile
if not os.path.isdir(DATASET_FILEPATH):
    with zipfile.ZipFile(DATASET_ZIP, 'r') as zip_ref:
        zip_ref.extractall(DATASET_FOLDER)

df_meta = pd.read_json(METADATA_FILEPATH)

"""## Data Description

Data description
The data is provided in 'train.zip', which contains two parts:
- train.json: metadata for each claim in the training set
- train_articles: folder of related articles for all the claims in the training set

In 'train.json', the following fields are provided for each claim (NB: some fields might be empty for some claims):
-  “claim”: statement
    Example: "claim": "Viral posts claim that climate change is a \"made-up catastrophe.\""
-  “claimant”: entity who made the claim Example: "claimant": "Social media posts"
-  “date”: when the claim was made Example: "date": "2019-05-08"
-  **“label”: truth labeling of the claim (0:false, 1:partly true, 2:true) Example: "label": 0**
-  “related_articles”: list of source and supporting article ids (that point to files in train_articles). Source articles denote articles that contain the claim, although the claim may be phrased differently in the article. Supporting articles denote articles that provide evidence that support the rating of the claim. Source and supporting articles are not distinguished in the list.
    Example: "related_articles": [20540, 38088, 14596, 20539, 20546, 38393, 20541, 41754]
-  “id”: unique identifier for each claim
    Example: “id”: 5

The folder 'train_articles' contains text files where the name of each text file is an id to
associate the article with a specific claim in 'train.json'. 

Example: 20540.txt

# Problem


In today’s society, misinformation is everywhere. From hoaxes passed around on social media, to misrepresented scientific studies, to fiction posted on shady websites to get clicks. Misinformation makes it challenging for people to know what is really going on in the world, prevents citizens from making informed decisions and ultimately jeopardizes the integrity of the democratic system all over the world.
 
For many years, human fact-checkers have reliably debunked false reports by conducting independent research to validate or disprove controversial claims. Unfortunately, fact-checking is an expensive, time-intensive process and by the time a hoax has been disproved, it has already spread through social media and been accepted as truth by a disconcertingly large number of people.
 
With the advancement of machine learning and natural language processing techniques, we have an opportunity to fact-check claims faster than ever and stop fake news before it spreads. 

**The goal is to use artificial intelligence to automate the fact-checking process and flag whether a claim is true (label=2), mostly true (label=1) or false (label=0).**

# Exploratory Data Analysis

In order to know more about this dataset, we will explore the target column (label) as well as  gain any insights that we can from rest of the fields. This is important so we can extract better features for our models.


Firstly, we already know the details about the dataset as mentioned in Data Description so we won't dive deep into that.

### Lets check for any missing or null values.
"""

df_meta.head()

df_meta.info()

"""From the first few rows of the dataset, we notice some values for claimants being blank. By checking the info we confirm that they are not null. Lets assign "NA" to all the blank values for claimants.
#### Replacing empty Claimant values with "NA"
"""

df_meta = df_meta.replace(r'^\s*$', "NA", regex=True)

"""Next, lets investigate the target class i.e. label

#### Frequency Distribution
"""

def change_width(ax, new_value) :
    for patch in ax.patches :
        current_width = patch.get_width()
        diff = current_width - new_value

        # we change the bar width
        patch.set_width(new_value)

        # we recenter the bar
        patch.set_x(patch.get_x() + diff * .5)

fig, ax = plt.subplots()

labels=df_meta.label.value_counts().tolist()
x=["Fake","Partly True","True"]
y=labels

sns.barplot(x,y,ax=ax,palette="rocket")



change_width(ax, .50)
plt.xticks(fontsize=14)
plt.xlabel("Labels",size=18)
plt.ylabel("No. Of Claims",size=18)
plt.show()

from IPython.display import display, Image
display(Image("Picture1.png"))

"""There is a class imbalance for the target class. Therefore we can either run some oversampling, undersampling or ensembling techniques to resolve this issue or use models that don't predict or depend too much on the majority value of the target class.

## Analysis of Claimant Label Wise

There are samples with no claimant. Let's analyze the data on the basis of claimant
"""

# top 25 claimants
df_meta['claimant'].value_counts()[:25]

"""**No. of Null/None Values in total are pretty huge approx 5000**

###  Count of Claimants for Fake news
"""

fig, ax = plt.subplots(figsize=(8,4))

claimants_fake=df_meta[["claimant","label"]][df_meta[["claimant","label"]]["label"]==0]["claimant"].value_counts()
fake_dict=dict(claimants_fake)
list(fake_dict.keys())
list(fake_dict.values())
print("Total number of claimants that exist for Top 10 Fake news are = ",sum(list(fake_dict.values())))
print ("Total number of claimants actually present are =  4374")
#plt.figure(figsize=(8,6.5))
sb.barplot(list(fake_dict.keys())[:10],list(fake_dict.values())[:10])


change_width(ax, .90)
plt.xticks(size = 12, rotation = 45)
#plt.yticks()
plt.title("Count Plot for Top 10 Claimants for Fake Claims")
#plt.legend()
plt.ylabel("Count",size=20)
plt.xlabel("Claimants",size=20)
plt.show()

"""Most false claims are made by unnamed claimants like Bloggers, Various websites, multiple sources etc.
Majority of false claims have no claimants (i.e. NA). 
Lets compare claimants present versus total claimants in the dataset:
"""

fig, ax = plt.subplots(figsize = (5,3))
sb.barplot(["Claimants Actually Present", "Total Claimants"],[4374,7408])
change_width(ax, .50)
plt.ylabel("Count",size=15)
plt.xlabel("Claimants",size=15)
plt.title("Samples with actual claimants", size = 10)

"""###  Count of Claimants for Partly True news"""

fig, ax = plt.subplots(figsize = (8,4))

claimants_partly_true=df_meta[["claimant","label"]][df_meta[["claimant","label"]]["label"]==1]["claimant"].value_counts()
partly_true_dict=dict(claimants_partly_true)
list(partly_true_dict.keys())
list(partly_true_dict.values())

print("Total number of claimants that exist for Partly True news are = ",sum(list(partly_true_dict.values())))
print ("Total number of claimants actually present are =  5164")

sb.barplot(list(partly_true_dict.keys())[:10],list(partly_true_dict.values())[:10] )

change_width(ax, 0.90)
plt.title("Count Plot for Claimants for Top 10 Partly True Claims")
plt.xticks(size=12, rotation = 30)
plt.yticks(size=12)
#plt.legend()
plt.ylabel("Count",size=20)
plt.xlabel("Claimants",size=20)
plt.show()

"""There are more claimants with names. Lets compare the claimants presence in this category:"""

fig, ax = plt.subplots(figsize = (5,3))

sb.barplot(["Claimants Actually Present", "Total Claimants"],[5164,6451])
change_width(ax, .50)
plt.ylabel("Count",size=15)
plt.xlabel("Claimants",size=15)
plt.title("Samples with actual claimants", size = 10)

fig, ax = plt.subplots(figsize = (14,4))

claimants_true=df_meta[["claimant","label"]][df_meta[["claimant","label"]]["label"]==2]["claimant"].value_counts()
true_dict=dict(claimants_true)
list(true_dict.keys())
list(true_dict.values())

print("Total number of claimants that exist for Fake news are = ",sum(list(true_dict.values())))
print ("Total number of claimants actually present are =  1055")


sb.barplot(list(true_dict.keys())[:10],list(true_dict.values())[:10])

change_width(ax, 0.90)
plt.xticks(size=12, rotation = 30)
plt.yticks(size=12)
plt.title("Count Plot for Top 10 Claimants for True Claims")
#plt.legend()
plt.ylabel("Count",size=20)
plt.xlabel("Claimants",size=20)
plt.show()

"""We notice no unnamed claimants in the top 10 of true claims."""

fig, ax = plt.subplots(figsize = (5,3))

sb.barplot(["Claimants Actually Present", "Total Claimants"],[1055,1696])
change_width(ax, .50)
plt.ylabel("Count",size=15)
plt.xlabel("Claimants",size=15)
plt.title("Samples with actual claimants", size = 10)

"""### Analysis of Related Articles

#### Number of Related articles per claim
"""

articles_length = df_meta['related_articles'].apply(lambda x: len(x)).value_counts()
fig, ax = plt.subplots(figsize = (10,5))

articles_length_dict=dict(articles_length)
list(articles_length_dict.keys())
list(articles_length_dict.values())


sb.barplot(list(articles_length_dict.keys())[:30],list(articles_length_dict.values())[:30] )

change_width(ax, 0.90)
plt.title("Count Plot for Top 20 Number of Related Articles per claim")
plt.xticks(size=12, rotation = 0)
plt.yticks(size=12)
#plt.legend()
plt.ylabel("Number of Claims",size=20)
plt.xlabel("Number of Related Articles",size=20)
plt.show()

"""We observe there are some claims that have related articles that can go as high as 57.

## Preprocessing of data

Before diving deeper into the claims and articles text, lets preprocess it by removing stop words, punctuation etc
"""

df = df_meta

def remove_punctuation(input_text):
    input_text = input_text.lower()
    translation_table = dict.fromkeys(map(ord, st.punctuation), ' ')
    output_text = input_text.translate(translation_table)
    output_text = re.sub('([^\w^\s]+)','',output_text)
    return output_text

def lemmatize_sent(input_text):
    lemmatizer=WordNetLemmatizer()
    sent_split = input_text.split()
    #print(sent_split)
    output_text = ' '.join(lemmatizer.lemmatize(word) for word in sent_split)
    #print(output_text)
    return output_text

def remove_stopwords(input_text):
    pattern = re.compile(r'\b(' + r'|'.join(stopwords.words('english')) + r')\b\s*')
    output_text = pattern.sub('', input_text)
    return output_text

def preprocess_claims(input_text):
    sentence = lemmatize_sent(input_text)
    output_text = remove_stopwords(remove_punctuation(input_text))
    return output_text



def preprocess_articles(articles_list):
    all_sentences_list = []
    for article in articles_list:
        article_text = ""
        with open(os.path.join(ARTICLES_FILEPATH, '%d.txt' % article), encoding="utf8") as f:
            article_text = f.read()
        sentences_list = tokenize.sent_tokenize(article_text)
        sentences_list = list(map(lambda x:re.sub('([\n]+)',' ',x),sentences_list))
        sentences_list = list(map(lambda x: preprocess_claims(x),sentences_list))
        all_sentences_list = all_sentences_list + sentences_list
    return str(" ".join(all_sentences_list))
#    return all_sentences_list


def load_articles(articles_list):
    #print(articles_list)
    all_sentences_list = []
    for article in articles_list:
        sencentces_list = []
        sencentces_list.append(df_pre.preprocessed_articles['%d.txt' % article])
        all_sentences_list = all_sentences_list + sencentces_list
    #print(all_sentences_list)
    return str(" ".join(all_sentences_list))

if enablePreprocessing:
    df.loc[:,"preprocessed_claims"] = df["claim"].apply(lambda x:preprocess_claims(x)).copy()
    df.loc[:,"preprocessed_articles"] = df["related_articles"].apply(lambda x:preprocess_articles(x)).copy()
else:
    #df_pre = pd.read_csv(PREPROCESSED_ARTICLES, index_col='article_id')
    #df.loc[:,"preprocessed_articles"] = df["related_articles"].apply(lambda x:load_articles(x)).copy()
    df = pd.read_csv(ALL_PREPROCESSED)

"""## Word Cloud"""

def split_sentences(input_text):
    wordslist=[]
    for text in input_text:
        wordslist.append(text.split())
    new_list=[]
    for list in wordslist:
        new_list=new_list+list
    output_text=" ".join(new_list)
    return output_text

"""**Fake News Word Analysis**"""

fake=df[df["label"]== 0]
fake_articles=fake["preprocessed_articles"].tolist()
fake_claims=fake["claim"].tolist()

stopwords=pd.read_csv("stop_words.txt", names = ['words'])['words'].to_list()

"""### Frequent Words in  Claims and Articles Associated with Fake Labels (0)"""

if enableWordCloudGeneration:
    fig, ax = plt.subplots(nrows=2, ncols=2, figsize = (15,15))

    #plt.figure(figsize=(30,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                    background_color ='white',  
                    min_font_size = 5,stopwords=stopwords).generate(split_sentences(fake_articles[:1500]))


    #plt.subplot(2,2,1)
    ax[0,0].imshow(wordcloud)
    ax[0,0].set_title('Word Cloud for Articles Without Stop Words')
    #ax[0,0].set_xlabel("Without Stop Words")
    #ax[0,0].axis("off")
    #ax[0,0].tight_layout(pad = 0) 
    #plt.show()

    #plt.figure(figsize=(20,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                    background_color ='white',  
                    min_font_size = 5,).generate(split_sentences(fake_articles[:1500]))



    ax[0,1].imshow(wordcloud)
    ax[0,1].set_title('Word Cloud for Articles With Stop Words')
    #ax[0,1].xlabel("With Stop Words")
    #ax[0,1].axis("off")
    #ax[0,1].tight_layout(pad = 0) 
    #plt.show()


    #plt.figure(figsize=(20,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                    background_color ='white',  
                    min_font_size = 5,stopwords=stopwords).generate(split_sentences(fake_claims[:1500]))


    #plt.subplot(2,2,3)
    ax[1,0].imshow(wordcloud)
    ax[1,0].set_title('Word Cloud for Claims Without Stop Words')
    #ax[1,0].xlabel("With Stop Words")
    #ax[1,0].axis("off")
    #ax[1,0].tight_layout(pad = 0) 
    #ax[1,0].show()


    #plt.figure(figsize=(20,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                    background_color ='white',  
                    min_font_size = 5,).generate(split_sentences(fake_claims[:1500]))


    #plt.subplot(2,2,4)
    ax[1,1].imshow(wordcloud)
    ax[1,1].set_title('Word Cloud for Claims With Stop Words')
    #ax[1,1].xlabel("With Stop Words")
    #ax[1,1].axis("off")
    #ax[1,1].tight_layout(pad = 0) 
    #plt.show()
    plt.tight_layout(pad = 0) 
else:
    from IPython.display import display, Image
    display(Image("images/wc_0.png"))

"""### Frequent Words in  Claims and Articles Associated with Partly True Labels (1)"""

partly_true=df[df["label"]== 1]
partly_true_articles=partly_true["preprocessed_articles"].tolist()
partly_true_claims=partly_true["claim"].tolist()

if enableWordCloudGeneration:
    fig, ax = plt.subplots(nrows=2, ncols=2, figsize = (15,15))

    #plt.figure(figsize=(30,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                    colormap='magma',  
                    min_font_size = 5,stopwords=stopwords).generate(split_sentences(partly_true_articles[:1500]))


    #plt.subplot(2,2,1)
    ax[0,0].imshow(wordcloud)
    ax[0,0].set_title('Word Cloud for Articles Without Stop Words',size=13)
    #ax[0,0].set_xlabel("Without Stop Words")
    #ax[0,0].axis("off")

    #plt.show()

    #plt.figure(figsize=(20,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                    colormap='magma',  
                    min_font_size = 5,).generate(split_sentences(partly_true_articles[:1500]))


    #ax[0,1].subplot(2,2,2)
    ax[0,1].imshow(wordcloud)
    ax[0,1].set_title('Word Cloud for Articles With Stop Words',size=13)
    #ax[0,1].xlabel("With Stop Words")
    #ax[0,1].axis("off")

    #plt.show()


    #plt.figure(figsize=(20,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                    colormap='magma', 
                    min_font_size = 5,stopwords=stopwords).generate(split_sentences(partly_true_claims[:1500]))


    #plt.subplot(2,2,3)
    ax[1,0].imshow(wordcloud)
    ax[1,0].set_title('Word Cloud for Claims Without Stop Words',size=13)
    #ax[1,0].xlabel("With Stop Words")
    #ax[1,0].axis("off")

    #ax[1,0].show()


    #plt.figure(figsize=(20,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                   colormap='magma', 
                    min_font_size = 5,).generate(split_sentences(partly_true_claims[:1500]))


    #plt.subplot(2,2,4)
    ax[1,1].imshow(wordcloud)
    ax[1,1].set_title('Word Cloud for Claims With Stop Words',size=13)
    #ax[1,1].xlabel("With Stop Words")
    #ax[1,1].axis("off")

    #plt.show()
    #plt.tight_layout(pad = 0) 
else:
    from IPython.display import display, Image
    display(Image("images/wc_1.png"))

"""### Frequent Words in  Claims and Articles Associated with True Labels (2)"""

true=df[df["label"]==2]
true_articles=true["preprocessed_articles"].tolist()
true_claims=true["claim"].tolist()

if enableWordCloudGeneration:
    fig, ax = plt.subplots(nrows=2, ncols=2, figsize = (15,15))

    #plt.figure(figsize=(30,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                    background_color ='pink',  
                    min_font_size = 5,stopwords=stopwords).generate(split_sentences(true_articles[:1500]))


    #plt.subplot(2,2,1)
    ax[0,0].imshow(wordcloud)
    ax[0,0].set_title('Word Cloud for True Articles Without Stop Words',size=13)
    #ax[0,0].set_xlabel("Without Stop Words")
    #ax[0,0].axis("off")

    #plt.show()

    #plt.figure(figsize=(20,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                    background_color ='pink',  
                    min_font_size = 5,).generate(split_sentences(true_articles[:1500]))


    #ax[0,1].subplot(2,2,2)
    ax[0,1].imshow(wordcloud)
    ax[0,1].set_title('Word Cloud for True Articles With Stop Words',size=13)
    #ax[0,1].xlabel("With Stop Words")
    #ax[0,1].axis("off")

    #plt.show()


    #plt.figure(figsize=(20,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                    background_color ='pink',  
                    min_font_size = 5,stopwords=stopwords).generate(split_sentences(true_claims[:1500]))


    #plt.subplot(2,2,3)
    ax[1,0].imshow(wordcloud)
    ax[1,0].set_title('Word Cloud for True Claims Without Stop Words',size=13)
    #ax[1,0].xlabel("With Stop Words")
    #ax[1,0].axis("off")

    #ax[1,0].show()


    #plt.figure(figsize=(20,10))
    wordcloud = WordCloud(width = 1500, height = 800, 
                    background_color ='pink',  
                    min_font_size = 5,).generate(split_sentences(true_claims[:1500]))


    #plt.subplot(2,2,4)
    ax[1,1].imshow(wordcloud)
    ax[1,1].set_title('Word Cloud for True Claims With Stop Words',size=13)
    #ax[1,1].xlabel("With Stop Words")
    #ax[1,1].axis("off")

    #plt.show()
    #plt.tight_layout(pad = 0) 
    plt.axis("off")
else:
    from IPython.display import display, Image
    display(Image("images/wc_2.png"))

def remove_punctuation(input_text):
    input_text = input_text.lower()
    translation_table = dict.fromkeys(map(ord, st.punctuation), ' ')
    output_text = input_text.translate(translation_table)
    output_text = re.sub('([^\w^\s]+)','',output_text)
    return output_text

def remove_stopwords(input_text):
    pattern = re.compile(r'\b(' + r'|'.join(stopwords.words('english')) + r')\b\s*')
    output_text = pattern.sub('', input_text)
    return output_text

def preprocess_claims(input_text):
    return remove_stopwords(remove_punctuation(input_text))

def get_ngrams(input_text,n):
    ngrams_list = []
    for ngram in ngrams(input_text.split(),n):
        ngrams_list.append(ngram)
    return ngrams_list

def load_articles(articles_list):
    #print(articles_list)
    all_sentences_list = []
    for article in articles_list:
        sencentces_list = []
        sencentces_list.append(df_pre.preprocessed_articles['%d.txt' % article])
        all_sentences_list = all_sentences_list + sencentces_list
    #print(all_sentences_list)
    return str(" ".join(all_sentences_list))
#    return all_sentences_list

def get_bigrams_count(c_bigram,a_bigram):
    count = 0
    for t in c_bigram:
        if t in a_bigram:
            count = count + 1
    return count

def get_word_count(str_articles, str_claim):
    if len([str_claim.strip()]) < 1:
        return [0]
    vectorizer = CountVectorizer()
    corpus = [str_claim]
    # learn the vocabulary and store CountVectorizer sparse matrix in X
    X = vectorizer.fit_transform(corpus)
    # columns of X correspond to the result of this method
    #vectorizer.get_feature_names() 
    # retrieving the matrix in the numpy form
    np_word_count = vectorizer.transform([str_articles]).toarray()
    #print(str_claim)
    #print(np_word_count)
    #ordered_word_count = []
    #for word in str_claim.split():
    #    if len(word)==1:
   #         continue
   #     index = vectorizer.vocabulary_.get(word)
    #    ordered_word_count.append(np_word_count[0][index])
        #print(word)
    #print(ordered_word_count)
    return np_word_count[0].tolist()

def get_missing_word_count(claim_len,bow_article):
    missing = 0
    #print("-----------------------------------------")
    #print(bow_article)
    #print(claim_len)
    for bow in bow_article:
        if bow==0:
            missing = missing + 1
    
    return round(missing/claim_len,3)

def get_bigram_present_count(bigram_article):
    present = 0
    for bigram in bigram_article:
        if bigram!=0:
            present = present + 1
    
    return round(present/len(bigram_article),3)

def get_trigram_present_count(trigram_article):
    present = 0
    for trigram in trigram_article:
        if trigram!=0:
            present = present + 1
    return round(present/len(trigram_article),3)

def get_lemmatized_str(words_str):
    #print(words_str)
    lemmatizer=WordNetLemmatizer()
    lemmatized_words_str = ' '.join([lemmatizer.lemmatize(w) for w in words_str.split()])
    #print(lemmatized_words_str)
    return lemmatized_words_str

def get_bigram_count(str_articles, str_claim):
    if len([str_claim.strip()]) < 2:
        return [0]
    vectorizer = CountVectorizer(ngram_range=(2,2))
    corpus = [str_claim]
    # learn the vocabulary and store CountVectorizer sparse matrix in X
    X = vectorizer.fit_transform(corpus)
    # columns of X correspond to the result of this method
    #vectorizer.get_feature_names() 
    # retrieving the matrix in the numpy form
    #print(vectorizer.get_feature_names())
    np_bigram_count = vectorizer.transform([str_articles]).toarray()
    return np_bigram_count[0].tolist()

def get_trigram_count(str_articles, str_claim):
    if len([str_claim.strip()]) < 2:
        return [0]
    vectorizer = CountVectorizer(ngram_range=(3,3))
    corpus = [str_claim]
    # learn the vocabulary and store CountVectorizer sparse matrix in X
    X = vectorizer.fit_transform(corpus)
    # columns of X correspond to the result of this method
    #vectorizer.get_feature_names() 
    # retrieving the matrix in the numpy form
    #print(vectorizer.get_feature_names())
    np_trigram_count = vectorizer.transform([str_articles]).toarray()
    return np_trigram_count[0].tolist()

def tfidf_word_count(str_articles, str_claim):
    #print(str_claim)
    corpus = [str_claim]
    vect = TfidfVectorizer()
    #print(corpus)
    X = vect.fit(corpus)
    weighted_word_count = X.transform([str_articles,str_claim])
    #print(X)
    #df_ = pd.DataFrame(weighted_word_count, 
                 # columns= vectorizer.get_feature_names(), 
                 # index=['str_articles', 'str_claim'])
    #print(df_)
    return weighted_word_count

def cosine_sim(str_articles, str_claim):
    a = tfidf_word_count(str_articles, str_claim)
    #print(a.shape)
    #print(type(a))
    #print((a.T).shape)
    #print(np.linalg.norm(a))
    #similarity_array = np.dot(a.T,a)/(np.linalg.norm(a))**2
    similarity_array = cosine_similarity(a[0,:],a[1,:])
    #similarity_array = cosine_similarity(str_articles,str_claim)
    return similarity_array

if enablePreprocessing:
    df.loc[:,"articles_len"] = df['related_articles'].apply(lambda x: len(x))
    df.loc[:,"articles_bow"] = df.apply(lambda x: get_word_count(x['preprocessed_articles'], x['preprocessed_claims']), axis=1).copy()
    df.loc[:,"articles_bigram_bow"] = df.apply(lambda x: get_bigram_count(x['preprocessed_articles'], x['preprocessed_claims']), axis=1).copy()
    df.loc[:,"articles_trigram_bow"] = df.apply(lambda x: get_trigram_count(x['preprocessed_articles'], x['preprocessed_claims']), axis=1).copy()
    df["claim_plus_articles"] = df["claim"] + df["preprocessed_articles"]
    df.loc[:,"f_unnamed_claimant"] = df["claimant"].apply(lambda x: 1 if (x=="Bloggers" or x=="Viral image" 
                                                          or x=="Various websites" or x=="Facebook posts"
                                                          or x=="multiple sources" or x=="Multiple sources"
                                                            or x=="NA" or x=="Viral meme" or x=="AFP Fact Check"
                                                        or x=="Social media posts")
                                                          else 0)
    df.loc[:,"f_bow_sum"] = df["articles_bow"].apply(sum)
    df.loc[:,"f_bow_avg"] = df["articles_bow"].apply(np.mean)
    df.loc[:,"f_bigram_bow_sum"] = df["articles_bigram_bow"].apply(sum)
    df.loc[:,"f_bigram_bow_avg"] = df["articles_bigram_bow"].apply(np.mean)
    df.loc[:,"f_trigram_bow_sum"] = df["articles_trigram_bow"].apply(sum)
    df.loc[:,"f_trigram_bow_avg"] = df["articles_trigram_bow"].apply(np.mean)
    df.loc[:,"f_c_percent_bigram_present"] = df.apply(lambda x: get_bigram_present_count(x['articles_bigram_bow']), axis=1)
    df.loc[:,"f_c_percent_trigram_present"] = df.apply(lambda x: get_trigram_present_count(x['articles_trigram_bow']), axis=1)
    df.loc[:,"f_c_wc"] = df["preprocessed_claims"].apply(lambda x: len([w for w in x.split()]))
    df.loc[:,"f_c_percent_missing"] = df.apply(lambda x: get_missing_word_count(x['f_c_wc'], x['articles_bow']), axis=1)
    df.loc[:,"f_a_len"] = df['related_articles'].apply(len)
    df.loc[:,"f_claimant_exists"] = df["claimant"].apply(lambda x: 0 if x=="NA" else 1)
    #df.loc[:,"f_cosine_similarity"] = df.apply(lambda x: cosine_sim(x['preprocessed_articles'], x['preprocessed_claims']), axis=1)

    df.loc[:,"f_cosine_similarity"] = df.apply(lambda x: cosine_sim(x['preprocessed_articles'], x['claim']), axis=1)
    df["f_cosine_similarity"]=df["f_cosine_similarity"].apply(lambda x:np.asscalar(x) )
    df["f_cosine_similarity"]=df["f_cosine_similarity"].apply(lambda x:round(x,1) )
    df[["f_cosine_similarity"]].fillna(0, inplace=True)

df.loc[:,'claim_plus_articles'] = df.claim + df.preprocessed_articles

columns_of_interest = ["claim","preprocessed_claims","label","articles_bow",
                       "f_bow_sum",# Sum of BOW of preprocessed claim tokens in all articles
                        "f_bigram_bow_sum",
            "f_bigram_bow_avg",
            "f_trigram_bow_sum",
            "f_trigram_bow_avg",
            "f_c_percent_bigram_present",
            "f_c_percent_trigram_present",
                       "f_bow_avg",# Average of BOW of preprocessed claim tokens in all articles
                       "f_c_wc",  # Count of preprocessed claim tokens
                       "f_a_len", # Count of related articles
                       "f_c_percent_missing", # Percent of tokens in preprocessed claim missing in all articles
                       "f_claimant_exists",
                       "f_cosine_similarity",
                       "f_unnamed_claimant"
                      ]
features = ["f_bow_sum",# Sum of BOW of preprocessed claim tokens in all articles
            "f_bow_avg",# Average of BOW of preprocessed claim tokens in all articles
            "f_bigram_bow_sum",
            "f_bigram_bow_avg",
            "f_trigram_bow_sum",
            "f_trigram_bow_avg",
            "f_c_percent_bigram_present",
            "f_c_percent_trigram_present",
            "f_claimant_exists",
            "f_cosine_similarity",
            "f_unnamed_claimant",
            "f_c_wc", # Count of preprocessed claim tokens
            "f_a_len", # Count of related articles
            "f_c_percent_missing", # Percent of tokens in preprocessed claim missing in all articles
            "f_claimant_exists" # 1 if Claimant exists else 0
           ]

"""## Models Preparation"""

df_staged = pd.DataFrame()
df_staged['Text'] = df["claim_plus_articles"]
df_staged['Label'] = df["label"]

X_train, X_test, y_train, y_test = train_test_split(df_staged['Text'], df_staged['Label'], test_size=0.2, random_state=7)

if enableTraining:
    tfidf_vectorizer = TfidfVectorizer(stop_words="english",ngram_range=(1, 3),lowercase=True,max_features=2000,max_df=0.7,min_df=0.1) 

    # Fit and transform the training data 
    tfidf1_train = tfidf_vectorizer.fit_transform(X_train)

    tfidf1_test = tfidf_vectorizer.transform(X_test)
else:
    tfidf1_test = scipy.sparse.load_npz('tfidf1_test.npz')
    tfidf1_train = scipy.sparse.load_npz('tfidf1_train.npz')

#scipy.sparse.save_npz('tfidf1_train.npz', tfidf1_train)
#scipy.sparse.save_npz('tfidf1_test.npz', tfidf1_test)

"""## Model Implementation

### Passive Aggressive
"""

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

if enableTraining:
    pac=PassiveAggressiveClassifier(max_iter=100,C=0.1)
    pac.fit(tfidf1_train,y_train)
    #joblib.dump(pac, './pac.pkl') 
else:
    pac = joblib.load('./pac.pkl')

pac = joblib.load('./pac.pkl') 
predictions = pac.predict(tfidf1_test)
from sklearn.metrics import classification_report
print(classification_report(predictions,y_test))



"""### 3 Feature Extraction  +  Model Implementation

### Passive Agressive Algorithm

### Features are Extracted using different techniques  and each set of technique is applied on Passive Agressive Model

Dropping all the claimants with nan values. Since claims with nan values cant be replaced by anything because there is nothing we can find out about that what could be the claimants be. and withot having claimants how can we justify the fact that they are fake or real. once way can be if they do not have claimants they can be classified as fake ones.

### Feature 1 - Claimant Features + Tfidf
"""

df=pd.read_csv("Fully Preprocessed articles.csv")

df=df.dropna(subset=['claimant'])

df.claimant.isnull().value_counts()

df.claimant.unique()[:100]

df1=pd.get_dummies(df,columns=["claimant"])

"""Feature EXtracted based upon claimants

### Train Test Split
"""

X=df1
y = df1.label
#y = y.astype('str')
X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size=0.2)
X_train.head()

tfidf_vectorizer = TfidfVectorizer(stop_words="english",ngram_range=(1, 1),lowercase=True,max_features=1000) 

# Fit and transform the training data 
tfidf1_train = tfidf_vectorizer.fit_transform(X_train["claim_plus_articles"]) 

# Transform the test set 
tfidf1_test = tfidf_vectorizer.transform(X_test["claim_plus_articles"])

features_train=np.concatenate((tfidf1_train.toarray(),X_train.iloc[:,1:].to_numpy()),axis=1)
features_test=np.concatenate((tfidf1_test.toarray(),X_test.iloc[:,1:].to_numpy()),axis=1)

features_train=pd.DataFrame(features_train)
features_test=pd.DataFrame(features_test)

features_train=features_train.fillna(0,axis=1)
features_test=features_test.fillna(0,axis=1)

features_train

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report



pac=PassiveAggressiveClassifier(max_iter=100,C=0.1)

pac.fit(features_train,Y_train)


y_pred=pac.predict(features_test)

y_predtrain=pac.predict(features_train)


print(classification_report(Y_test,y_pred))
print(classification_report(Y_train,y_predtrain))

print("train accuracy=",accuracy_score(Y_train,y_predtrain))
print("test accuracy=",accuracy_score(Y_test,y_pred))

"""This give score of 43 i have runned it once it worked but this need to be worked upon further this can help"""



"""### Feature 2 - Tfidf of Concatenation of claims and articles with N-gram(1,3)"""

df=pd.read_csv("Fully Preprocessed articles.csv")

df["claim_plus_articles"] = df["claim"] + df["all_preprocessed_articles"]

X=df[["claim_plus_articles"]]
y = df.label
#y = y.astype('str')
X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size=0.05)
X_train.head()

tfidf_vectorizer = TfidfVectorizer(stop_words="english",ngram_range=(1, 3),lowercase=True,max_features=2000) 

# Fit and transform the training data 
tfidf1_train = tfidf_vectorizer.fit_transform(X_train["claim_plus_articles"]) 

# Transform the test set 
tfidf1_test = tfidf_vectorizer.transform(X_test["claim_plus_articles"])

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score


pac=PassiveAggressiveClassifier(max_iter=600,C=0.1)

pac.fit(tfidf1_train,Y_train)


y_pred=pac.predict(tfidf1_test)

y_predtrain=pac.predict(tfidf1_train)


print(classification_report(Y_test,y_pred))
print(classification_report(Y_train,y_predtrain))

print("train accuracy=",accuracy_score(Y_train,y_predtrain))
print("test accuracy=",accuracy_score(Y_test,y_pred))
print("F1 Score=",f1_score(Y_test,y_pred, average='macro'))

"""### Feature 2- Tfidf with N-gram(1,2)"""

tfidf_vectorizer = TfidfVectorizer(stop_words="english",ngram_range=(1, 2),lowercase=True,max_features=2000) 

# Fit and transform the training data 
tfidf1_train = tfidf_vectorizer.fit_transform(X_train["claim_plus_articles"]) 

# Transform the test set 
tfidf1_test = tfidf_vectorizer.transform(X_test["claim_plus_articles"])

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score


pac=PassiveAggressiveClassifier(max_iter=500,C=0.1)

pac.fit(tfidf1_train,Y_train)


y_pred=pac.predict(tfidf1_test)

y_predtrain=pac.predict(tfidf1_train)


print(classification_report(Y_test,y_pred))
print(classification_report(Y_train,y_predtrain))

print("train accuracy=",accuracy_score(Y_train,y_predtrain))
print("test accuracy=",accuracy_score(Y_test,y_pred))
print("F1 Score=",f1_score(Y_test,y_pred, average='macro'))

"""### Feature 3 -Tfidf with N-gram(1,1)"""

tfidf_vectorizer = TfidfVectorizer(stop_words="english",ngram_range=(1, 1),lowercase=True,max_features=2000) 

# Fit and transform the training data 
tfidf1_train = tfidf_vectorizer.fit_transform(X_train["claim_plus_articles"]) 

# Transform the test set 
tfidf1_test = tfidf_vectorizer.transform(X_test["claim_plus_articles"])

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score


pac=PassiveAggressiveClassifier(max_iter=500,C=0.1)

pac.fit(tfidf1_train,Y_train)


y_pred=pac.predict(tfidf1_test)

y_predtrain=pac.predict(tfidf1_train)


print(classification_report(Y_test,y_pred))
print(classification_report(Y_train,y_predtrain))

print("train accuracy=",accuracy_score(Y_train,y_predtrain))
print("test accuracy=",accuracy_score(Y_test,y_pred))
print("F1 Score=",f1_score(Y_test,y_pred, average='macro'))

"""### Feature 5 -Tfidf with N-gram(2,3)"""

tfidf_vectorizer = TfidfVectorizer(stop_words="english",ngram_range=(2, 3),lowercase=True,max_features=2000) 

# Fit and transform the training data 
tfidf1_train = tfidf_vectorizer.fit_transform(X_train["claim_plus_articles"]) 

# Transform the test set 
tfidf1_test = tfidf_vectorizer.transform(X_test["claim_plus_articles"])

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score


pac=PassiveAggressiveClassifier(max_iter=500,C=0.1)

pac.fit(tfidf1_train,Y_train)


y_pred=pac.predict(tfidf1_test)

y_predtrain=pac.predict(tfidf1_train)


print(classification_report(Y_test,y_pred))
print(classification_report(Y_train,y_predtrain))

print("train accuracy=",accuracy_score(Y_train,y_predtrain))
print("test accuracy=",accuracy_score(Y_test,y_pred))
print("F1 Score=",f1_score(Y_test,y_pred, average='macro'))

"""### Feature 4- Sentence based extraction  using Cosine Similarity + TFIDF"""

similarity=pd.read_csv("features_similarity_data.csv")

similarity=similarity.dropna(subset=["extracted_sentences"])

X=similarity[["extracted_sentences"]]
y = similarity.label
#y = y.astype('str')
X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size=0.05)
X_train.head()

tfidf_vectorizer = TfidfVectorizer(stop_words="english",ngram_range=(1, 2),lowercase=True,max_features=2000) 

# Fit and transform the training data 
tfidf1_train = tfidf_vectorizer.fit_transform(X_train["extracted_sentences"]) 

# Transform the test set 
tfidf1_test = tfidf_vectorizer.transform(X_test["extracted_sentences"])

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score


pac=PassiveAggressiveClassifier(max_iter=500,C=0.1)

pac.fit(tfidf1_train,Y_train)


y_pred=pac.predict(tfidf1_test)

y_predtrain=pac.predict(tfidf1_train)


print(classification_report(Y_test,y_pred))
print(classification_report(Y_train,y_predtrain))

print("train accuracy=",accuracy_score(Y_train,y_predtrain))
print("test accuracy=",accuracy_score(Y_test,y_pred))
print("F1 Score=",f1_score(Y_test,y_pred, average='macro'))

"""### Feature 5- Count Vectorizer - Tokens from concartenation of claims + articles"""

X=df[["claim_plus_articles"]]
y = df.label
#y = y.astype('str')
X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size=0.05)
X_train.shape

from sklearn.feature_extraction.text import CountVectorizer
cv_vectorizer = CountVectorizer(stop_words="english",ngram_range=(1, 1),lowercase=True,max_features=2000) 

# Fit and transform the training data 
cv_train = cv_vectorizer.fit_transform(X_train["claim_plus_articles"]) 

# Transform the test set 
cv_test = cv_vectorizer.transform(X_test["claim_plus_articles"])

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score


pac=PassiveAggressiveClassifier(max_iter=500,C=0.1)

pac.fit(cv_train,Y_train)


y_pred=pac.predict(cv_test)

y_predtrain=pac.predict(cv_train)


print(classification_report(Y_test,y_pred))
print(classification_report(Y_train,y_predtrain))

print("train accuracy=",accuracy_score(Y_train,y_predtrain))
print("test accuracy=",accuracy_score(Y_test,y_pred))
print("F1 Score=",f1_score(Y_test,y_pred, average='macro'))

"""### LSTM"""

from keras import backend as K

def recall_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

def precision_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

max_len = 150
max_words = 1000
tok = Tokenizer(num_words = max_words)

if enableTraining:
    tok.fit_on_texts(x_train)
    sequences = tok.texts_to_sequences(x_train)
    sequences_matrix = sequence.pad_sequences(sequences,maxlen = max_len)
    def fake_news_detector():
        inputs = Input(name = 'inputs', shape = [max_len])
        layer = Embedding(max_words, 50, input_length = max_len)(inputs)
        layer = LSTM(64)(layer)
        layer = Dense(256, name = 'FC1')(layer)
        layer = Activation('relu')(layer)
        layer = Dropout(0.5)(layer)
        layer = Dense(1, name = 'out_layer')(layer)
        layer = Activation('sigmoid')(layer)
        model = Model(inputs = inputs, outputs=layer)
        return model

    model = fake_news_detector()
    model.summary()
    model.compile(loss = 'binary_crossentropy',
                 optimizer = RMSprop(),
                 metrics=['acc',f1_m,precision_m, recall_m])
    model.fit(sequences_matrix,y_train,batch_size=128,epochs=10,
         validation_split=0.2,
         callbacks=[EarlyStopping(monitor='val_loss',
                                  min_delta=0.0001)])
    #model.save("lstm.model")

if not enableTraining:
    lstm_model =  load_model('lstm.model',custom_objects={"f1_m": f1_m,"precision_m": precision_m, "recall_m": recall_m})

test_sequences = tok.texts_to_sequences(X_test)
test_sequences_matrix = sequence.pad_sequences(test_sequences,
                                              maxlen = max_len)

metrics = lstm_model.evaluate(test_sequences_matrix, y_test)

print("Test set \n Loss : {:0.3f} \n Accuracy: {:0.3f} \n F1-Score: {:0.3f}". format(metrics[0],metrics[1],metrics[2]))

"""### BERT

Extracting features for BERT
"""

from numpy import *

def tfidf_word_count(str_articles, str_claim):
    #print(str_claim)
    corpus = [str_claim]
    vectorizer = TfidfVectorizer()
    #print(corpus)
    X = vectorizer.fit(corpus)
    weighted_word_count = X.transform([str_articles, str_claim])
    #print(X)
    #df_ = pd.DataFrame(weighted_word_count, 
                 # columns= vectorizer.get_feature_names(), 
                 # index=['str_articles', 'str_claim'])
    #print(df_)
    return weighted_word_count

def cosine_sim(str_articles, str_claim):
    a = tfidf_word_count(str_articles, str_claim)
    #print(a)
    #print((a.T).shape)
    #print(np.linalg.norm(a))
    #similarity_array = np.dot(a.T,a)/(np.linalg.norm(a))**2
    similarity_array = cs(a[1,:],a[0,:])
    similarity_array = pd.DataFrame(similarity_array)
    similarity_array.fillna(0, inplace = True)
    #print(type(similarity_array))
    cs_value = similarity_array[0][0]
    #print(cs_value)
    return cs_value.astype(float)

def extract_sent(row, input_list):
    similarity_values = []
    for i in input_list:
        similarity_values.append(cosine_sim(i, df.preprocessed_claims[row]))
    extract_sent = []
    for i,val in enumerate(similarity_values):
        if val != 0:
            index = similarity_values.index(val)
            extract_sent.append(input_list[index])
            similarity_values[i] = 0
    return extract_sent

if runBERT:
    sentences = []
    for row in range(df.shape[0]):
        b = ''
        a = extract_sent(row, df.preprocessed_articles[row])
        b = ' '.join(a)
        sentences.append(b)

if runBERT:
    df.loc[:,"extracted_sentences"] = sentences

if runBERT:
    df.to_csv('features_similarity_data.csv')

"""Using claims and articles together as a feature, which is further tokenized for processing in BERT"""

if runBERT:
    import datetime
    import json
    import os
    import pprint
    import random
    import string
    import sys
    import tensorflow as tf

    assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'
    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']
    print('TPU address is', TPU_ADDRESS)

    from google.colab import auth
    auth.authenticate_user()
    with tf.Session(TPU_ADDRESS) as session:
      print('TPU devices:')
      pprint.pprint(session.list_devices())

      # Upload credentials to TPU.
      with open('/content/adc.json', 'r') as f:
        auth_info = json.load(f)
      tf.contrib.cloud.configure_gcs(session, credentials=auth_info)
      # Now credentials are set for all future sessions on this TPU.

if runBERT:
    import sys

    !test -d bert_repo || git clone https://github.com/google-research/bert bert_repo
    if not 'bert_repo' in sys.path:
      sys.path += ['bert_repo']

    # import python modules defined by BERT
    import modeling
    import optimization
    import run_classifier
    import run_classifier_with_tfhub
    import tokenization
    import pandas as pd
    from sklearn.model_selection import train_test_split

    # import tfhub 
    import tensorflow_hub as hub

if runBERT:
    #using file from drive
    !pip install -U -q PyDrive
    from pydrive.auth import GoogleAuth
    from pydrive.drive import GoogleDrive
    from google.colab import auth
    from oauth2client.client import GoogleCredentials
    # Authenticate and create the PyDrive client.
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)
    link = 'https://drive.google.com/open?id=1toB0cpmnLwhOZg1IPrZ0p1IsEgpP--sg' # The shareable link
    fluff, id = link.split('=')
    df['extracted_sentences'].fillna('0', inplace= True)
    df['text'] =  df.preprocessed_articles + df.preprocessed_claims

#downloaded = drive.CreateFile({'id':id}) 
#downloaded.GetContentFile('features_similarity_data.csv')  
#df = pd.read_csv('features_similarity_data.csv')

#dict(df[['label', 'extracted_sentences']].head(5))

df.head()

if runBERT:
    TASK = 'MRPC' #@param {type:"string"}
    assert TASK in ('MRPC', 'CoLA'), 'Only (MRPC, CoLA) are demonstrated here.'

    # Download glue data.
    #! test -d download_glue_repo || git clone https://gist.github.com/60c2bdb54d156a41194446737ce03e2e.git download_glue_repo
    #!python download_glue_repo/download_glue_data.py --data_dir='glue_data' --tasks=$TASK

    #TASK_DATA_DIR = 'Files/' + TASK
    #print('***** Task data directory: {} *****'.format(TASK_DATA_DIR))
    #!ls $TASK_DATA_DIR

    BUCKET = '__' #@param {type:"string"}
    assert BUCKET, 'Must specify an existing GCS bucket name'
    OUTPUT_DIR = 'gs://{}/bert-tfhub/models/{}'.format(BUCKET, TASK)
    tf.gfile.MakeDirs(OUTPUT_DIR)
    print('***** Model output directory: {} *****'.format(OUTPUT_DIR))

    # Available pretrained model checkpoints:
    #   uncased_L-12_H-768_A-12: uncased BERT base model
    #   uncased_L-24_H-1024_A-16: uncased BERT large model
    #   cased_L-12_H-768_A-12: cased BERT large model
    BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:"string"}
    BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'

if runBERT:
    tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)
    tokenizer.tokenize("This here's an example of using the BERT tokenizer")

if runBERT:
    df = df.sample(frac = 1)
    X_train, X_test, y_train, y_test = train_test_split(df["text"], df["label"], test_size=0.3, random_state=42)

X_test.shape

def create_examples(lines, set_type, labels=None):
#Generate data for the BERT model
    guid = f'{set_type}'
    examples = []
    if guid == 'train':
        for line, label in zip(lines, labels):
            text_a = line
            label = str(label)
            examples.append(
              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
    else:
        for line in lines:
            text_a = line
            label = '0'
            examples.append(
              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
    return examples

if runBERT:
    TRAIN_BATCH_SIZE = 32
    EVAL_BATCH_SIZE = 8
    PREDICT_BATCH_SIZE = 8
    LEARNING_RATE = 2e-5
    NUM_TRAIN_EPOCHS = 50
    MAX_SEQ_LENGTH = 200
    # Warmup is a period of time where hte learning rate 
    # is small and gradually increases--usually helps training.
    WARMUP_PROPORTION = 0.1
    # Model configs
    SAVE_CHECKPOINTS_STEPS = 1000
    SAVE_SUMMARY_STEPS = 500

    #processors = {
    #  "cola": run_classifier.ColaProcessor,
    #  "mnli": run_classifier.MnliProcessor,
    #  "mrpc": run_classifier.MrpcProcessor,
    #}
    #processor = processors[TASK.lower()]()
    label_list = [str(num) for num in range(3)]

    # Compute number of train and warmup steps from batch size
    #train_examples = processor.get_train_examples(X_train)
    train_examples = create_examples(X_train, 'train', labels=y_train)
    num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)
    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)

    # Setup TPU related config
    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)
    NUM_TPU_CORES = 8
    ITERATIONS_PER_LOOP = 1000

    def get_run_config(output_dir):
      return tf.contrib.tpu.RunConfig(
        cluster=tpu_cluster_resolver,
        model_dir=output_dir,
        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,
        tpu_config=tf.contrib.tpu.TPUConfig(
            iterations_per_loop=ITERATIONS_PER_LOOP,
            num_shards=NUM_TPU_CORES,
            per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))

if runBERT:
    # Force TF Hub writes to the GS bucket we provide.
    os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR

    model_fn = run_classifier_with_tfhub.model_fn_builder(
      num_labels=len(label_list),
      learning_rate=LEARNING_RATE,
      num_train_steps=num_train_steps,
      num_warmup_steps=num_warmup_steps,
      use_tpu=True,
      bert_hub_module_handle=BERT_MODEL_HUB
    )

    estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(
      use_tpu=True,
      model_fn=model_fn,
      config=get_run_config(OUTPUT_DIR),
      train_batch_size=TRAIN_BATCH_SIZE,
      eval_batch_size=EVAL_BATCH_SIZE,
      predict_batch_size=PREDICT_BATCH_SIZE,
    )

    estimator = tf.contrib.tpu.TPUEstimator(
     use_tpu=True,
      model_fn=model_fn,
      config=get_run_config(OUTPUT_DIR),
      train_batch_size=TRAIN_BATCH_SIZE,
      eval_batch_size=EVAL_BATCH_SIZE,
      predict_batch_size=PREDICT_BATCH_SIZE,
      )

# Train the model
def model_train(estimator):
  print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')
  # We'll set sequences to be at most 128 tokens long.
  train_features = run_classifier.convert_examples_to_features(
      train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)
  print('***** Started training at {} *****'.format(datetime.datetime.now()))
  print('  Num examples = {}'.format(len(train_examples)))
  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))
  tf.logging.info("  Num steps = %d", num_train_steps)
  train_input_fn = run_classifier.input_fn_builder(
      features=train_features,
      seq_length=MAX_SEQ_LENGTH,
      is_training=True,
      drop_remainder=True)
  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)
  print('***** Finished training at {} *****'.format(datetime.datetime.now()))

if runBERT:
    model_train(estimator_from_tfhub)



def model_predict(estimator):
  # Make predictions on a subset of eval examples
  prediction_examples = create_examples(X_test, 'test')
  input_features = run_classifier.convert_examples_to_features(prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer)
  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)
  predictions = estimator.predict(predict_input_fn)

  for example, prediction in zip(prediction_examples, predictions):
    print('text_a: %s\ntext_b: %s\nlabel:%s\nprediction:%s\n' % (example.text_a, example.text_b, str(example.label), prediction['probabilities']))

if runBERT:
    model_predict(estimator_from_tfhub)

if runBERT:
    prediction_examples = create_examples(X_test, 'test')
    input_features = run_classifier.convert_examples_to_features(prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer)
    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)
    predictions = estimator.predict(predict_input_fn)

import numpy as np

if runBERT:
    preds = []
    for prediction in predictions:
          preds.append(np.argmax(prediction['probabilities']))

from sklearn.metrics import accuracy_score
print("Accuracy of BERT is:",accuracy_score(y_test,preds))

from sklearn.metrics import classification_report
print(classification_report(y_test,preds))

"""Sentences in articles have been compared with the claim using cosine similarity. Those with similarity value greater than 0 are used as feature for analysis in BERT"""

if runBERT:
    import datetime
    import json
    import os
    import pprint
    import random
    import string
    import sys
    import tensorflow as tf

    assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'
    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']
    print('TPU address is', TPU_ADDRESS)

    from google.colab import auth
    auth.authenticate_user()
    with tf.Session(TPU_ADDRESS) as session:
      print('TPU devices:')
      pprint.pprint(session.list_devices())

      # Upload credentials to TPU.
      with open('/content/adc.json', 'r') as f:
        auth_info = json.load(f)
      tf.contrib.cloud.configure_gcs(session, credentials=auth_info)
      # Now credentials are set for all future sessions on this TPU.

if runBERT:
    import sys

    !test -d bert_repo || git clone https://github.com/google-research/bert bert_repo
    if not 'bert_repo' in sys.path:
      sys.path += ['bert_repo']

    # import python modules defined by BERT
    import modeling
    import optimization
    import run_classifier
    import run_classifier_with_tfhub
    import tokenization
    import pandas as pd
    from sklearn.model_selection import train_test_split

    # import tfhub 
    import tensorflow_hub as hub

if runBERT:
    #using file from drive
    !pip install -U -q PyDrive
    from pydrive.auth import GoogleAuth
    from pydrive.drive import GoogleDrive
    from google.colab import auth
    from oauth2client.client import GoogleCredentials
    # Authenticate and create the PyDrive client.
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)
    link = 'https://drive.google.com/open?id=1toB0cpmnLwhOZg1IPrZ0p1IsEgpP--sg' # The shareable link
    fluff, id = link.split('=')
    downloaded = drive.CreateFile({'id':id}) 
    downloaded.GetContentFile('features_similarity_data.csv')  
    df = pd.read_csv('features_similarity_data.csv')

#dict(df[['label', 'extracted_sentences']].head(5))

if runBERT:
    df['extracted_sentences'].fillna('0', inplace= True)

df['text'] =  df.extracted_sentences
df.head()

if runBERT:
    TASK = 'MRPC' #@param {type:"string"}
    assert TASK in ('MRPC', 'CoLA'), 'Only (MRPC, CoLA) are demonstrated here.'

    # Download glue data.
    #! test -d download_glue_repo || git clone https://gist.github.com/60c2bdb54d156a41194446737ce03e2e.git download_glue_repo
    #!python download_glue_repo/download_glue_data.py --data_dir='glue_data' --tasks=$TASK

    #TASK_DATA_DIR = 'Files/' + TASK
    #print('***** Task data directory: {} *****'.format(TASK_DATA_DIR))
    #!ls $TASK_DATA_DIR

    BUCKET = 'fake_fact' #@param {type:"string"}
    assert BUCKET, 'Must specify an existing GCS bucket name'
    OUTPUT_DIR = 'gs://{}/bert-tfhub/models/{}'.format(BUCKET, TASK)
    tf.gfile.MakeDirs(OUTPUT_DIR)
    print('***** Model output directory: {} *****'.format(OUTPUT_DIR))

    # Available pretrained model checkpoints:
    #   uncased_L-12_H-768_A-12: uncased BERT base model
    #   uncased_L-24_H-1024_A-16: uncased BERT large model
    #   cased_L-12_H-768_A-12: cased BERT large model
    BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:"string"}
    BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'

if runBERT:
    tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)
    tokenizer.tokenize("This here's an example of using the BERT tokenizer")

if runBERT:
    df = df.sample(frac = 1)
    X_train, X_test, y_train, y_test = train_test_split(df["text"], df["label"], test_size=0.3, random_state=42)

X_test.shape

df['text'].value_counts(dropna = False)

def create_examples(lines, set_type, labels=None):
#Generate data for the BERT model
    guid = f'{set_type}'
    examples = []
    if guid == 'train':
        for line, label in zip(lines, labels):
            text_a = line
            label = str(label)
            examples.append(
              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
    else:
        for line in lines:
            text_a = line
            label = '0'
            examples.append(
              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
    return examples

if runBERT:
    TRAIN_BATCH_SIZE = 32
    EVAL_BATCH_SIZE = 8
    PREDICT_BATCH_SIZE = 8
    LEARNING_RATE = 2e-5
    NUM_TRAIN_EPOCHS = 100
    MAX_SEQ_LENGTH = 60
    # Warmup is a period of time where hte learning rate 
    # is small and gradually increases--usually helps training.
    WARMUP_PROPORTION = 0.1
    # Model configs
    SAVE_CHECKPOINTS_STEPS = 1000
    SAVE_SUMMARY_STEPS = 500

    #processors = {
    #  "cola": run_classifier.ColaProcessor,
    #  "mnli": run_classifier.MnliProcessor,
    #  "mrpc": run_classifier.MrpcProcessor,
    #}
    #processor = processors[TASK.lower()]()
    label_list = [str(num) for num in range(3)]

    # Compute number of train and warmup steps from batch size
    #train_examples = processor.get_train_examples(X_train)
    train_examples = create_examples(X_train, 'train', labels=y_train)
    num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)
    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)

    # Setup TPU related config
    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)
    NUM_TPU_CORES = 8
    ITERATIONS_PER_LOOP = 1000

    def get_run_config(output_dir):
      return tf.contrib.tpu.RunConfig(
        cluster=tpu_cluster_resolver,
        model_dir=output_dir,
        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,
        tpu_config=tf.contrib.tpu.TPUConfig(
            iterations_per_loop=ITERATIONS_PER_LOOP,
            num_shards=NUM_TPU_CORES,
            per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))

if runBERT:
    # Force TF Hub writes to the GS bucket we provide.
    os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR

    model_fn = run_classifier_with_tfhub.model_fn_builder(
      num_labels=len(label_list),
      learning_rate=LEARNING_RATE,
      num_train_steps=num_train_steps,
      num_warmup_steps=num_warmup_steps,
      use_tpu=True,
      bert_hub_module_handle=BERT_MODEL_HUB
    )

    estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(
      use_tpu=True,
      model_fn=model_fn,
      config=get_run_config(OUTPUT_DIR),
      train_batch_size=TRAIN_BATCH_SIZE,
      eval_batch_size=EVAL_BATCH_SIZE,
      predict_batch_size=PREDICT_BATCH_SIZE,
    )

    estimator = tf.contrib.tpu.TPUEstimator(
     use_tpu=True,
      model_fn=model_fn,
      config=get_run_config(OUTPUT_DIR),
      train_batch_size=TRAIN_BATCH_SIZE,
      eval_batch_size=EVAL_BATCH_SIZE,
      predict_batch_size=PREDICT_BATCH_SIZE,
      )



# Train the model
def model_train(estimator):
  print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')
  # We'll set sequences to be at most 128 tokens long.
  train_features = run_classifier.convert_examples_to_features(
      train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)
  print('***** Started training at {} *****'.format(datetime.datetime.now()))
  print('  Num examples = {}'.format(len(train_examples)))
  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))
  tf.logging.info("  Num steps = %d", num_train_steps)
  train_input_fn = run_classifier.input_fn_builder(
      features=train_features,
      seq_length=MAX_SEQ_LENGTH,
      is_training=True,
      drop_remainder=True)
  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)
  print('***** Finished training at {} *****'.format(datetime.datetime.now()))

if runBERT:
    model_train(estimator_from_tfhub)



if runBERT:
    prediction_examples = create_examples(X_test, 'test')
    input_features = run_classifier.convert_examples_to_features(prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer)
    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)
    predictions = estimator.predict(predict_input_fn)

import numpy as np

if runBERT:
    preds = []
    for prediction in predictions:
          preds.append(np.argmax(prediction['probabilities']))

from sklearn.metrics import accuracy_score
print("Accuracy of BERT is:",accuracy_score(y_test,preds))

from sklearn.metrics import classification_report
print(classification_report(y_test,preds))

predict_input_fn

if runBERT:
    #prediction_examples = create_examples(X_train, 'train')
    input_features = run_classifier.convert_examples_to_features(train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)
    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)
    predictions_train = estimator.predict(predict_input_fn)

if runBERT:
    preds_train = []
    for prediction in predictions_train:
          preds_train.append(np.argmax(prediction['probabilities']))



from sklearn.metrics import accuracy_score
print("Accuracy of BERT is:",accuracy_score(y_train,preds_train))

from sklearn.metrics import classification_report
print(classification_report(y_train,preds_train))





"""## Models Performance"""



"""### F1-score vs ALL features Techniques for passive agressive"""

def change_width(ax, new_value) :
    for patch in ax.patches :
        current_width = patch.get_width()
        diff = current_width - new_value

        # we change the bar width
        patch.set_width(new_value)

        # we recenter the bar
        patch.set_x(patch.get_x() + diff * .5)

scores=[32,55,77,43.2,47.5,45.29,45.09,43,39,43.34,45]

features=["LSTM","BERT","BERT(Cosine)","PA-Claimants","PA-Tfidf(1,3)","PA-Tfidf(1,2)","PA-Tfidf(1,1)","PA-Tfidf(2,3)","PA-Tfidf(sentence)","PA-BOW","Hash Vectorizer"]

fig, ax = plt.subplots(figsize=(27,12))

sb.barplot(features,scores)
change_width(ax, 0.40)               
plt.xlabel("Models",size=40)
plt.ylabel("F1-Scores",size=40)
plt.xticks(size=25,rotation = 45)
plt.yticks(size=30)
plt.ylim([0, 80])
plt.title("F1 score for Different Models",size=50)

train=[62,76,81.4,62,72.3,65,77.3,74.8,68.2,61.2,70]
test=[54,63,80.9,55,60.1,58,55.2,56.6,53.36,52.8,58]

import pandas as pd
acc=pd.DataFrame({"train":[62,76,81.4,62,72.3,65,77.3,74.8,68.2,61.2,70],"test":[54,63,80.9,55,60.1,58,55.2,56.6,53.36,52.8,58]},
                 index=["LSTM","BERT","BERT(Cosine)","PA-Claimants","PA-Tfidf(1,3)",
                        "PA-Tfidf(1,2)","PA-Tfidf(1,1)","PA-Tfidf(2,3)","PA-Tfidf(sentence)","PA-BOW","Hash Vectorizer"])

acc

plt.figure(figsize=(20,12))
ax=acc.plot.bar(rot=45,figsize=(27,12),title="Train & Test Accuracy for Different Models",)
ax.set_title("Train & Test Accuracy for Different Models",size=40)
ax.set_xlabel("Models",size=40)
ax.set_ylabel("Accuracy",size=40)

plt.xticks(size=25)
plt.yticks(size=25)
plt.legend(loc="best", prop={'size': 30})

"""## References

- Leaders Prize: Fact or Fake News? https://leadersprize.truenorthwaterloo.com/en/ [accessed on November 26, 2019)
"""

